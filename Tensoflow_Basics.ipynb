{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow Introduction:\n",
    "Tensorflow is a machine learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = 100000\n",
    "xs = np.random.uniform(-10,10,size=(observation,1))\n",
    "zs = np.random.uniform(-10,10,(observation,1))\n",
    "\n",
    "gen_inputs = np.column_stack((xs,zs))\n",
    "\n",
    "noise = np.random.uniform(-1,1,(observation,1))\n",
    "gen_target = 2*xs - 3*zs + 5 + noise\n",
    "\n",
    "np.savez('TF_Intro', inputs = gen_inputs, targets = gen_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving tensor flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kaori\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.placeholder(tf.float32,[None, input_size])\n",
    "targets = tf.placeholder(tf.float32,[None, output_size])\n",
    "\n",
    "weights = tf.Variable(tf.random_uniform([input_size, output_size], minval=-0.1, maxval=0.1))\n",
    "biases = tf.Variable(tf.random_uniform([output_size], minval=-0.1, maxval=0.1))\n",
    "\n",
    "outputs = tf.matmul(inputs,weights) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the objective function and optimization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kaori\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "mean_loss = tf.losses.mean_squared_error(labels=targets, predictions=outputs) / 2.\n",
    "\n",
    "optimize = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(mean_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation for The Execution of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.global_variables_initializer()\n",
    "sess.run(initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('TF_Intro.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning ofthe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in the iteration 0 is  243.034\n",
      "Loss in the iteration 1 is  227.8216\n",
      "Loss in the iteration 2 is  213.61232\n",
      "Loss in the iteration 3 is  200.3398\n",
      "Loss in the iteration 4 is  187.94225\n",
      "Loss in the iteration 5 is  176.3618\n",
      "Loss in the iteration 6 is  165.54463\n",
      "Loss in the iteration 7 is  155.44025\n",
      "Loss in the iteration 8 is  146.00157\n",
      "Loss in the iteration 9 is  137.18465\n",
      "Loss in the iteration 10 is  128.94844\n",
      "Loss in the iteration 11 is  121.25456\n",
      "Loss in the iteration 12 is  114.0672\n",
      "Loss in the iteration 13 is  107.35292\n",
      "Loss in the iteration 14 is  101.0805\n",
      "Loss in the iteration 15 is  95.22072\n",
      "Loss in the iteration 16 is  89.74635\n",
      "Loss in the iteration 17 is  84.63194\n",
      "Loss in the iteration 18 is  79.85368\n",
      "Loss in the iteration 19 is  75.38943\n",
      "Loss in the iteration 20 is  71.21842\n",
      "Loss in the iteration 21 is  67.32131\n",
      "Loss in the iteration 22 is  63.67999\n",
      "Loss in the iteration 23 is  60.27756\n",
      "Loss in the iteration 24 is  57.09826\n",
      "Loss in the iteration 25 is  54.12739\n",
      "Loss in the iteration 26 is  51.351124\n",
      "Loss in the iteration 27 is  48.75666\n",
      "Loss in the iteration 28 is  46.331974\n",
      "Loss in the iteration 29 is  44.06587\n",
      "Loss in the iteration 30 is  41.94786\n",
      "Loss in the iteration 31 is  39.96817\n",
      "Loss in the iteration 32 is  38.117683\n",
      "Loss in the iteration 33 is  36.38785\n",
      "Loss in the iteration 34 is  34.770706\n",
      "Loss in the iteration 35 is  33.258823\n",
      "Loss in the iteration 36 is  31.845253\n",
      "Loss in the iteration 37 is  30.523495\n",
      "Loss in the iteration 38 is  29.287493\n",
      "Loss in the iteration 39 is  28.13159\n",
      "Loss in the iteration 40 is  27.05049\n",
      "Loss in the iteration 41 is  26.039267\n",
      "Loss in the iteration 42 is  25.09331\n",
      "Loss in the iteration 43 is  24.208296\n",
      "Loss in the iteration 44 is  23.380213\n",
      "Loss in the iteration 45 is  22.6053\n",
      "Loss in the iteration 46 is  21.88005\n",
      "Loss in the iteration 47 is  21.20119\n",
      "Loss in the iteration 48 is  20.565657\n",
      "Loss in the iteration 49 is  19.97059\n",
      "Loss in the iteration 50 is  19.413319\n",
      "Loss in the iteration 51 is  18.891352\n",
      "Loss in the iteration 52 is  18.402363\n",
      "Loss in the iteration 53 is  17.944176\n",
      "Loss in the iteration 54 is  17.514753\n",
      "Loss in the iteration 55 is  17.11221\n",
      "Loss in the iteration 56 is  16.734764\n",
      "Loss in the iteration 57 is  16.380766\n",
      "Loss in the iteration 58 is  16.048668\n",
      "Loss in the iteration 59 is  15.737025\n",
      "Loss in the iteration 60 is  15.444489\n",
      "Loss in the iteration 61 is  15.169805\n",
      "Loss in the iteration 62 is  14.911797\n",
      "Loss in the iteration 63 is  14.66936\n",
      "Loss in the iteration 64 is  14.441476\n",
      "Loss in the iteration 65 is  14.227182\n",
      "Loss in the iteration 66 is  14.025587\n",
      "Loss in the iteration 67 is  13.835853\n",
      "Loss in the iteration 68 is  13.6572\n",
      "Loss in the iteration 69 is  13.488896\n",
      "Loss in the iteration 70 is  13.330265\n",
      "Loss in the iteration 71 is  13.18067\n",
      "Loss in the iteration 72 is  13.039515\n",
      "Loss in the iteration 73 is  12.906245\n",
      "Loss in the iteration 74 is  12.780345\n",
      "Loss in the iteration 75 is  12.661332\n",
      "Loss in the iteration 76 is  12.548751\n",
      "Loss in the iteration 77 is  12.44218\n",
      "Loss in the iteration 78 is  12.341225\n",
      "Loss in the iteration 79 is  12.2455225\n",
      "Loss in the iteration 80 is  12.15472\n",
      "Loss in the iteration 81 is  12.068505\n",
      "Loss in the iteration 82 is  11.98657\n",
      "Loss in the iteration 83 is  11.908644\n",
      "Loss in the iteration 84 is  11.834457\n",
      "Loss in the iteration 85 is  11.76377\n",
      "Loss in the iteration 86 is  11.69635\n",
      "Loss in the iteration 87 is  11.631988\n",
      "Loss in the iteration 88 is  11.570485\n",
      "Loss in the iteration 89 is  11.51165\n",
      "Loss in the iteration 90 is  11.455314\n",
      "Loss in the iteration 91 is  11.401311\n",
      "Loss in the iteration 92 is  11.349496\n",
      "Loss in the iteration 93 is  11.299725\n",
      "Loss in the iteration 94 is  11.251862\n",
      "Loss in the iteration 95 is  11.205785\n",
      "Loss in the iteration 96 is  11.161378\n",
      "Loss in the iteration 97 is  11.11854\n",
      "Loss in the iteration 98 is  11.077163\n",
      "Loss in the iteration 99 is  11.037154\n",
      "Loss in the iteration 100 is  10.998428\n",
      "Loss in the iteration 101 is  10.960905\n",
      "Loss in the iteration 102 is  10.924501\n",
      "Loss in the iteration 103 is  10.8891535\n",
      "Loss in the iteration 104 is  10.854788\n",
      "Loss in the iteration 105 is  10.821345\n",
      "Loss in the iteration 106 is  10.788765\n",
      "Loss in the iteration 107 is  10.757\n",
      "Loss in the iteration 108 is  10.72599\n",
      "Loss in the iteration 109 is  10.695695\n",
      "Loss in the iteration 110 is  10.666067\n",
      "Loss in the iteration 111 is  10.637062\n",
      "Loss in the iteration 112 is  10.608647\n",
      "Loss in the iteration 113 is  10.580783\n",
      "Loss in the iteration 114 is  10.553437\n",
      "Loss in the iteration 115 is  10.526575\n",
      "Loss in the iteration 116 is  10.500172\n",
      "Loss in the iteration 117 is  10.474196\n",
      "Loss in the iteration 118 is  10.448625\n",
      "Loss in the iteration 119 is  10.423429\n",
      "Loss in the iteration 120 is  10.398595\n",
      "Loss in the iteration 121 is  10.374094\n",
      "Loss in the iteration 122 is  10.349911\n",
      "Loss in the iteration 123 is  10.326025\n",
      "Loss in the iteration 124 is  10.30242\n",
      "Loss in the iteration 125 is  10.279081\n",
      "Loss in the iteration 126 is  10.25599\n",
      "Loss in the iteration 127 is  10.233135\n",
      "Loss in the iteration 128 is  10.2105055\n",
      "Loss in the iteration 129 is  10.1880865\n",
      "Loss in the iteration 130 is  10.165866\n",
      "Loss in the iteration 131 is  10.143834\n",
      "Loss in the iteration 132 is  10.121984\n",
      "Loss in the iteration 133 is  10.100301\n",
      "Loss in the iteration 134 is  10.07878\n",
      "Loss in the iteration 135 is  10.05741\n",
      "Loss in the iteration 136 is  10.036187\n",
      "Loss in the iteration 137 is  10.015104\n",
      "Loss in the iteration 138 is  9.99415\n",
      "Loss in the iteration 139 is  9.973323\n",
      "Loss in the iteration 140 is  9.952616\n",
      "Loss in the iteration 141 is  9.932024\n",
      "Loss in the iteration 142 is  9.91154\n",
      "Loss in the iteration 143 is  9.89116\n",
      "Loss in the iteration 144 is  9.87088\n",
      "Loss in the iteration 145 is  9.8507\n",
      "Loss in the iteration 146 is  9.830608\n",
      "Loss in the iteration 147 is  9.810607\n",
      "Loss in the iteration 148 is  9.79069\n",
      "Loss in the iteration 149 is  9.770854\n",
      "Loss in the iteration 150 is  9.751099\n",
      "Loss in the iteration 151 is  9.73142\n",
      "Loss in the iteration 152 is  9.711814\n",
      "Loss in the iteration 153 is  9.69228\n",
      "Loss in the iteration 154 is  9.672813\n",
      "Loss in the iteration 155 is  9.653415\n",
      "Loss in the iteration 156 is  9.634083\n",
      "Loss in the iteration 157 is  9.614811\n",
      "Loss in the iteration 158 is  9.595604\n",
      "Loss in the iteration 159 is  9.576456\n",
      "Loss in the iteration 160 is  9.557366\n",
      "Loss in the iteration 161 is  9.53833\n",
      "Loss in the iteration 162 is  9.519352\n",
      "Loss in the iteration 163 is  9.500426\n",
      "Loss in the iteration 164 is  9.481556\n",
      "Loss in the iteration 165 is  9.462735\n",
      "Loss in the iteration 166 is  9.443968\n",
      "Loss in the iteration 167 is  9.425249\n",
      "Loss in the iteration 168 is  9.406579\n",
      "Loss in the iteration 169 is  9.387955\n",
      "Loss in the iteration 170 is  9.369383\n",
      "Loss in the iteration 171 is  9.350854\n",
      "Loss in the iteration 172 is  9.332372\n",
      "Loss in the iteration 173 is  9.313933\n",
      "Loss in the iteration 174 is  9.29554\n",
      "Loss in the iteration 175 is  9.277191\n",
      "Loss in the iteration 176 is  9.258888\n",
      "Loss in the iteration 177 is  9.240625\n",
      "Loss in the iteration 178 is  9.222405\n",
      "Loss in the iteration 179 is  9.2042265\n",
      "Loss in the iteration 180 is  9.186091\n",
      "Loss in the iteration 181 is  9.167994\n",
      "Loss in the iteration 182 is  9.149939\n",
      "Loss in the iteration 183 is  9.131925\n",
      "Loss in the iteration 184 is  9.113949\n",
      "Loss in the iteration 185 is  9.096013\n",
      "Loss in the iteration 186 is  9.078117\n",
      "Loss in the iteration 187 is  9.060259\n",
      "Loss in the iteration 188 is  9.04244\n",
      "Loss in the iteration 189 is  9.024659\n",
      "Loss in the iteration 190 is  9.006919\n",
      "Loss in the iteration 191 is  8.989213\n",
      "Loss in the iteration 192 is  8.971547\n",
      "Loss in the iteration 193 is  8.953917\n",
      "Loss in the iteration 194 is  8.936325\n",
      "Loss in the iteration 195 is  8.918771\n",
      "Loss in the iteration 196 is  8.901251\n",
      "Loss in the iteration 197 is  8.883769\n",
      "Loss in the iteration 198 is  8.866325\n",
      "Loss in the iteration 199 is  8.848915\n",
      "Loss in the iteration 200 is  8.831541\n",
      "Loss in the iteration 201 is  8.814206\n",
      "Loss in the iteration 202 is  8.796906\n",
      "Loss in the iteration 203 is  8.779638\n",
      "Loss in the iteration 204 is  8.762409\n",
      "Loss in the iteration 205 is  8.745215\n",
      "Loss in the iteration 206 is  8.728053\n",
      "Loss in the iteration 207 is  8.710929\n",
      "Loss in the iteration 208 is  8.693841\n",
      "Loss in the iteration 209 is  8.676784\n",
      "Loss in the iteration 210 is  8.659767\n",
      "Loss in the iteration 211 is  8.64278\n",
      "Loss in the iteration 212 is  8.625828\n",
      "Loss in the iteration 213 is  8.608913\n",
      "Loss in the iteration 214 is  8.59203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in the iteration 215 is  8.575184\n",
      "Loss in the iteration 216 is  8.55837\n",
      "Loss in the iteration 217 is  8.541592\n",
      "Loss in the iteration 218 is  8.524845\n",
      "Loss in the iteration 219 is  8.508131\n",
      "Loss in the iteration 220 is  8.491454\n",
      "Loss in the iteration 221 is  8.474811\n",
      "Loss in the iteration 222 is  8.458199\n",
      "Loss in the iteration 223 is  8.441623\n",
      "Loss in the iteration 224 is  8.4250765\n",
      "Loss in the iteration 225 is  8.408565\n",
      "Loss in the iteration 226 is  8.39209\n",
      "Loss in the iteration 227 is  8.375644\n",
      "Loss in the iteration 228 is  8.359232\n",
      "Loss in the iteration 229 is  8.342854\n",
      "Loss in the iteration 230 is  8.3265085\n",
      "Loss in the iteration 231 is  8.310196\n",
      "Loss in the iteration 232 is  8.293915\n",
      "Loss in the iteration 233 is  8.277669\n",
      "Loss in the iteration 234 is  8.261455\n",
      "Loss in the iteration 235 is  8.245272\n",
      "Loss in the iteration 236 is  8.229122\n",
      "Loss in the iteration 237 is  8.213008\n",
      "Loss in the iteration 238 is  8.196919\n",
      "Loss in the iteration 239 is  8.180867\n",
      "Loss in the iteration 240 is  8.164846\n",
      "Loss in the iteration 241 is  8.148861\n",
      "Loss in the iteration 242 is  8.132903\n",
      "Loss in the iteration 243 is  8.116978\n",
      "Loss in the iteration 244 is  8.101087\n",
      "Loss in the iteration 245 is  8.085225\n",
      "Loss in the iteration 246 is  8.069396\n",
      "Loss in the iteration 247 is  8.053597\n",
      "Loss in the iteration 248 is  8.037833\n",
      "Loss in the iteration 249 is  8.022099\n",
      "Loss in the iteration 250 is  8.006396\n",
      "Loss in the iteration 251 is  7.990725\n",
      "Loss in the iteration 252 is  7.975085\n",
      "Loss in the iteration 253 is  7.9594765\n",
      "Loss in the iteration 254 is  7.9439\n",
      "Loss in the iteration 255 is  7.928354\n",
      "Loss in the iteration 256 is  7.91284\n",
      "Loss in the iteration 257 is  7.897356\n",
      "Loss in the iteration 258 is  7.881901\n",
      "Loss in the iteration 259 is  7.86648\n",
      "Loss in the iteration 260 is  7.8510885\n",
      "Loss in the iteration 261 is  7.8357277\n",
      "Loss in the iteration 262 is  7.820399\n",
      "Loss in the iteration 263 is  7.8051014\n",
      "Loss in the iteration 264 is  7.789831\n",
      "Loss in the iteration 265 is  7.774595\n",
      "Loss in the iteration 266 is  7.759385\n",
      "Loss in the iteration 267 is  7.744209\n",
      "Loss in the iteration 268 is  7.7290626\n",
      "Loss in the iteration 269 is  7.7139473\n",
      "Loss in the iteration 270 is  7.6988606\n",
      "Loss in the iteration 271 is  7.6838055\n",
      "Loss in the iteration 272 is  7.6687794\n",
      "Loss in the iteration 273 is  7.6537824\n",
      "Loss in the iteration 274 is  7.6388187\n",
      "Loss in the iteration 275 is  7.6238813\n",
      "Loss in the iteration 276 is  7.608975\n",
      "Loss in the iteration 277 is  7.5941005\n",
      "Loss in the iteration 278 is  7.579253\n",
      "Loss in the iteration 279 is  7.5644355\n",
      "Loss in the iteration 280 is  7.54965\n",
      "Loss in the iteration 281 is  7.5348926\n",
      "Loss in the iteration 282 is  7.5201626\n",
      "Loss in the iteration 283 is  7.505465\n",
      "Loss in the iteration 284 is  7.4907956\n",
      "Loss in the iteration 285 is  7.4761558\n",
      "Loss in the iteration 286 is  7.4615436\n",
      "Loss in the iteration 287 is  7.446965\n",
      "Loss in the iteration 288 is  7.4324117\n",
      "Loss in the iteration 289 is  7.4178886\n",
      "Loss in the iteration 290 is  7.4033937\n",
      "Loss in the iteration 291 is  7.388931\n",
      "Loss in the iteration 292 is  7.374493\n",
      "Loss in the iteration 293 is  7.360085\n",
      "Loss in the iteration 294 is  7.3457065\n",
      "Loss in the iteration 295 is  7.331358\n",
      "Loss in the iteration 296 is  7.317036\n",
      "Loss in the iteration 297 is  7.3027425\n",
      "Loss in the iteration 298 is  7.288481\n",
      "Loss in the iteration 299 is  7.2742443\n",
      "Loss in the iteration 300 is  7.260038\n",
      "Loss in the iteration 301 is  7.24586\n",
      "Loss in the iteration 302 is  7.2317085\n",
      "Loss in the iteration 303 is  7.217585\n",
      "Loss in the iteration 304 is  7.203493\n",
      "Loss in the iteration 305 is  7.1894264\n",
      "Loss in the iteration 306 is  7.1753893\n",
      "Loss in the iteration 307 is  7.16138\n",
      "Loss in the iteration 308 is  7.1473975\n",
      "Loss in the iteration 309 is  7.1334457\n",
      "Loss in the iteration 310 is  7.1195188\n",
      "Loss in the iteration 311 is  7.1056223\n",
      "Loss in the iteration 312 is  7.091751\n",
      "Loss in the iteration 313 is  7.07791\n",
      "Loss in the iteration 314 is  7.064095\n",
      "Loss in the iteration 315 is  7.0503087\n",
      "Loss in the iteration 316 is  7.03655\n",
      "Loss in the iteration 317 is  7.0228176\n",
      "Loss in the iteration 318 is  7.0091133\n",
      "Loss in the iteration 319 is  6.9954376\n",
      "Loss in the iteration 320 is  6.9817863\n",
      "Loss in the iteration 321 is  6.9681625\n",
      "Loss in the iteration 322 is  6.954569\n",
      "Loss in the iteration 323 is  6.9410014\n",
      "Loss in the iteration 324 is  6.92746\n",
      "Loss in the iteration 325 is  6.9139476\n",
      "Loss in the iteration 326 is  6.90046\n",
      "Loss in the iteration 327 is  6.887001\n",
      "Loss in the iteration 328 is  6.873568\n",
      "Loss in the iteration 329 is  6.860162\n",
      "Loss in the iteration 330 is  6.8467813\n",
      "Loss in the iteration 331 is  6.83343\n",
      "Loss in the iteration 332 is  6.8201036\n",
      "Loss in the iteration 333 is  6.8068037\n",
      "Loss in the iteration 334 is  6.7935324\n",
      "Loss in the iteration 335 is  6.7802863\n",
      "Loss in the iteration 336 is  6.767065\n",
      "Loss in the iteration 337 is  6.753874\n",
      "Loss in the iteration 338 is  6.7407064\n",
      "Loss in the iteration 339 is  6.7275667\n",
      "Loss in the iteration 340 is  6.7144494\n",
      "Loss in the iteration 341 is  6.701363\n",
      "Loss in the iteration 342 is  6.688302\n",
      "Loss in the iteration 343 is  6.675265\n",
      "Loss in the iteration 344 is  6.6622562\n",
      "Loss in the iteration 345 is  6.6492724\n",
      "Loss in the iteration 346 is  6.636316\n",
      "Loss in the iteration 347 is  6.6233835\n",
      "Loss in the iteration 348 is  6.610478\n",
      "Loss in the iteration 349 is  6.597598\n",
      "Loss in the iteration 350 is  6.584744\n",
      "Loss in the iteration 351 is  6.571915\n",
      "Loss in the iteration 352 is  6.559111\n",
      "Loss in the iteration 353 is  6.5463333\n",
      "Loss in the iteration 354 is  6.5335836\n",
      "Loss in the iteration 355 is  6.5208564\n",
      "Loss in the iteration 356 is  6.508156\n",
      "Loss in the iteration 357 is  6.49548\n",
      "Loss in the iteration 358 is  6.4828315\n",
      "Loss in the iteration 359 is  6.470203\n",
      "Loss in the iteration 360 is  6.4576044\n",
      "Loss in the iteration 361 is  6.44503\n",
      "Loss in the iteration 362 is  6.43248\n",
      "Loss in the iteration 363 is  6.419956\n",
      "Loss in the iteration 364 is  6.4074574\n",
      "Loss in the iteration 365 is  6.3949833\n",
      "Loss in the iteration 366 is  6.3825336\n",
      "Loss in the iteration 367 is  6.370108\n",
      "Loss in the iteration 368 is  6.357711\n",
      "Loss in the iteration 369 is  6.3453336\n",
      "Loss in the iteration 370 is  6.332985\n",
      "Loss in the iteration 371 is  6.3206587\n",
      "Loss in the iteration 372 is  6.3083587\n",
      "Loss in the iteration 373 is  6.296081\n",
      "Loss in the iteration 374 is  6.2838306\n",
      "Loss in the iteration 375 is  6.2716036\n",
      "Loss in the iteration 376 is  6.2594\n",
      "Loss in the iteration 377 is  6.2472224\n",
      "Loss in the iteration 378 is  6.235069\n",
      "Loss in the iteration 379 is  6.222938\n",
      "Loss in the iteration 380 is  6.2108345\n",
      "Loss in the iteration 381 is  6.1987524\n",
      "Loss in the iteration 382 is  6.186696\n",
      "Loss in the iteration 383 is  6.1746626\n",
      "Loss in the iteration 384 is  6.162654\n",
      "Loss in the iteration 385 is  6.1506686\n",
      "Loss in the iteration 386 is  6.1387067\n",
      "Loss in the iteration 387 is  6.12677\n",
      "Loss in the iteration 388 is  6.1148567\n",
      "Loss in the iteration 389 is  6.102968\n",
      "Loss in the iteration 390 is  6.0911036\n",
      "Loss in the iteration 391 is  6.0792613\n",
      "Loss in the iteration 392 is  6.0674415\n",
      "Loss in the iteration 393 is  6.055649\n",
      "Loss in the iteration 394 is  6.0438776\n",
      "Loss in the iteration 395 is  6.0321283\n",
      "Loss in the iteration 396 is  6.020405\n",
      "Loss in the iteration 397 is  6.0087056\n",
      "Loss in the iteration 398 is  5.9970274\n",
      "Loss in the iteration 399 is  5.985374\n",
      "Loss in the iteration 400 is  5.973743\n",
      "Loss in the iteration 401 is  5.9621377\n",
      "Loss in the iteration 402 is  5.9505525\n",
      "Loss in the iteration 403 is  5.938991\n",
      "Loss in the iteration 404 is  5.9274526\n",
      "Loss in the iteration 405 is  5.915938\n",
      "Loss in the iteration 406 is  5.904446\n",
      "Loss in the iteration 407 is  5.8929763\n",
      "Loss in the iteration 408 is  5.8815312\n",
      "Loss in the iteration 409 is  5.8701077\n",
      "Loss in the iteration 410 is  5.8587093\n",
      "Loss in the iteration 411 is  5.847331\n",
      "Loss in the iteration 412 is  5.8359776\n",
      "Loss in the iteration 413 is  5.8246436\n",
      "Loss in the iteration 414 is  5.8133354\n",
      "Loss in the iteration 415 is  5.8020487\n",
      "Loss in the iteration 416 is  5.7907844\n",
      "Loss in the iteration 417 is  5.779543\n",
      "Loss in the iteration 418 is  5.768323\n",
      "Loss in the iteration 419 is  5.7571273\n",
      "Loss in the iteration 420 is  5.7459526\n",
      "Loss in the iteration 421 is  5.734802\n",
      "Loss in the iteration 422 is  5.7236733\n",
      "Loss in the iteration 423 is  5.7125654\n",
      "Loss in the iteration 424 is  5.70148\n",
      "Loss in the iteration 425 is  5.6904173\n",
      "Loss in the iteration 426 is  5.6793756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in the iteration 427 is  5.6683564\n",
      "Loss in the iteration 428 is  5.65736\n",
      "Loss in the iteration 429 is  5.6463857\n",
      "Loss in the iteration 430 is  5.635432\n",
      "Loss in the iteration 431 is  5.6245\n",
      "Loss in the iteration 432 is  5.613591\n",
      "Loss in the iteration 433 is  5.602705\n",
      "Loss in the iteration 434 is  5.5918393\n",
      "Loss in the iteration 435 is  5.5809946\n",
      "Loss in the iteration 436 is  5.5701723\n",
      "Loss in the iteration 437 is  5.559373\n",
      "Loss in the iteration 438 is  5.5485935\n",
      "Loss in the iteration 439 is  5.537836\n",
      "Loss in the iteration 440 is  5.527101\n",
      "Loss in the iteration 441 is  5.516386\n",
      "Loss in the iteration 442 is  5.5056925\n",
      "Loss in the iteration 443 is  5.4950213\n",
      "Loss in the iteration 444 is  5.4843717\n",
      "Loss in the iteration 445 is  5.473742\n",
      "Loss in the iteration 446 is  5.463135\n",
      "Loss in the iteration 447 is  5.452549\n",
      "Loss in the iteration 448 is  5.441982\n",
      "Loss in the iteration 449 is  5.431437\n",
      "Loss in the iteration 450 is  5.4209137\n",
      "Loss in the iteration 451 is  5.410412\n",
      "Loss in the iteration 452 is  5.3999314\n",
      "Loss in the iteration 453 is  5.389471\n",
      "Loss in the iteration 454 is  5.379031\n",
      "Loss in the iteration 455 is  5.3686123\n",
      "Loss in the iteration 456 is  5.3582163\n",
      "Loss in the iteration 457 is  5.347838\n",
      "Loss in the iteration 458 is  5.337483\n",
      "Loss in the iteration 459 is  5.3271475\n",
      "Loss in the iteration 460 is  5.316832\n",
      "Loss in the iteration 461 is  5.3065386\n",
      "Loss in the iteration 462 is  5.296264\n",
      "Loss in the iteration 463 is  5.286013\n",
      "Loss in the iteration 464 is  5.275779\n",
      "Loss in the iteration 465 is  5.265567\n",
      "Loss in the iteration 466 is  5.255375\n",
      "Loss in the iteration 467 is  5.2452054\n",
      "Loss in the iteration 468 is  5.2350535\n",
      "Loss in the iteration 469 is  5.2249236\n",
      "Loss in the iteration 470 is  5.2148113\n",
      "Loss in the iteration 471 is  5.2047215\n",
      "Loss in the iteration 472 is  5.194652\n",
      "Loss in the iteration 473 is  5.184603\n",
      "Loss in the iteration 474 is  5.1745725\n",
      "Loss in the iteration 475 is  5.1645637\n",
      "Loss in the iteration 476 is  5.1545725\n",
      "Loss in the iteration 477 is  5.1446033\n",
      "Loss in the iteration 478 is  5.1346526\n",
      "Loss in the iteration 479 is  5.1247244\n",
      "Loss in the iteration 480 is  5.1148133\n",
      "Loss in the iteration 481 is  5.1049237\n",
      "Loss in the iteration 482 is  5.0950527\n",
      "Loss in the iteration 483 is  5.0852017\n",
      "Loss in the iteration 484 is  5.0753713\n",
      "Loss in the iteration 485 is  5.06556\n",
      "Loss in the iteration 486 is  5.0557675\n",
      "Loss in the iteration 487 is  5.0459957\n",
      "Loss in the iteration 488 is  5.036242\n",
      "Loss in the iteration 489 is  5.026509\n",
      "Loss in the iteration 490 is  5.0167947\n",
      "Loss in the iteration 491 is  5.007102\n",
      "Loss in the iteration 492 is  4.9974256\n",
      "Loss in the iteration 493 is  4.98777\n",
      "Loss in the iteration 494 is  4.9781337\n",
      "Loss in the iteration 495 is  4.9685173\n",
      "Loss in the iteration 496 is  4.9589195\n",
      "Loss in the iteration 497 is  4.9493403\n",
      "Loss in the iteration 498 is  4.9397807\n",
      "Loss in the iteration 499 is  4.930242\n",
      "Loss in the iteration 500 is  4.920719\n",
      "Loss in the iteration 501 is  4.9112163\n",
      "Loss in the iteration 502 is  4.901734\n",
      "Loss in the iteration 503 is  4.8922696\n",
      "Loss in the iteration 504 is  4.8828235\n",
      "Loss in the iteration 505 is  4.873397\n",
      "Loss in the iteration 506 is  4.863989\n",
      "Loss in the iteration 507 is  4.8546004\n",
      "Loss in the iteration 508 is  4.84523\n",
      "Loss in the iteration 509 is  4.835878\n",
      "Loss in the iteration 510 is  4.8265452\n",
      "Loss in the iteration 511 is  4.8172326\n",
      "Loss in the iteration 512 is  4.807936\n",
      "Loss in the iteration 513 is  4.7986608\n",
      "Loss in the iteration 514 is  4.789401\n",
      "Loss in the iteration 515 is  4.7801614\n",
      "Loss in the iteration 516 is  4.77094\n",
      "Loss in the iteration 517 is  4.7617383\n",
      "Loss in the iteration 518 is  4.752553\n",
      "Loss in the iteration 519 is  4.7433877\n",
      "Loss in the iteration 520 is  4.7342386\n",
      "Loss in the iteration 521 is  4.7251096\n",
      "Loss in the iteration 522 is  4.7159986\n",
      "Loss in the iteration 523 is  4.7069054\n",
      "Loss in the iteration 524 is  4.69783\n",
      "Loss in the iteration 525 is  4.6887736\n",
      "Loss in the iteration 526 is  4.679735\n",
      "Loss in the iteration 527 is  4.6707144\n",
      "Loss in the iteration 528 is  4.6617117\n",
      "Loss in the iteration 529 is  4.6527276\n",
      "Loss in the iteration 530 is  4.643761\n",
      "Loss in the iteration 531 is  4.6348104\n",
      "Loss in the iteration 532 is  4.6258807\n",
      "Loss in the iteration 533 is  4.616968\n",
      "Loss in the iteration 534 is  4.6080737\n",
      "Loss in the iteration 535 is  4.599196\n",
      "Loss in the iteration 536 is  4.590336\n",
      "Loss in the iteration 537 is  4.581495\n",
      "Loss in the iteration 538 is  4.57267\n",
      "Loss in the iteration 539 is  4.5638633\n",
      "Loss in the iteration 540 is  4.555075\n",
      "Loss in the iteration 541 is  4.5463033\n",
      "Loss in the iteration 542 is  4.537549\n",
      "Loss in the iteration 543 is  4.5288134\n",
      "Loss in the iteration 544 is  4.5200944\n",
      "Loss in the iteration 545 is  4.511393\n",
      "Loss in the iteration 546 is  4.5027075\n",
      "Loss in the iteration 547 is  4.4940405\n",
      "Loss in the iteration 548 is  4.485392\n",
      "Loss in the iteration 549 is  4.4767604\n",
      "Loss in the iteration 550 is  4.468145\n",
      "Loss in the iteration 551 is  4.4595475\n",
      "Loss in the iteration 552 is  4.450967\n",
      "Loss in the iteration 553 is  4.4424057\n",
      "Loss in the iteration 554 is  4.4338584\n",
      "Loss in the iteration 555 is  4.425329\n",
      "Loss in the iteration 556 is  4.416817\n",
      "Loss in the iteration 557 is  4.4083223\n",
      "Loss in the iteration 558 is  4.3998446\n",
      "Loss in the iteration 559 is  4.3913827\n",
      "Loss in the iteration 560 is  4.382939\n",
      "Loss in the iteration 561 is  4.3745117\n",
      "Loss in the iteration 562 is  4.3661017\n",
      "Loss in the iteration 563 is  4.357708\n",
      "Loss in the iteration 564 is  4.3493304\n",
      "Loss in the iteration 565 is  4.3409715\n",
      "Loss in the iteration 566 is  4.3326273\n",
      "Loss in the iteration 567 is  4.3243012\n",
      "Loss in the iteration 568 is  4.315991\n",
      "Loss in the iteration 569 is  4.307698\n",
      "Loss in the iteration 570 is  4.2994194\n",
      "Loss in the iteration 571 is  4.29116\n",
      "Loss in the iteration 572 is  4.282916\n",
      "Loss in the iteration 573 is  4.274689\n",
      "Loss in the iteration 574 is  4.266476\n",
      "Loss in the iteration 575 is  4.2582827\n",
      "Loss in the iteration 576 is  4.2501044\n",
      "Loss in the iteration 577 is  4.2419424\n",
      "Loss in the iteration 578 is  4.233796\n",
      "Loss in the iteration 579 is  4.225668\n",
      "Loss in the iteration 580 is  4.2175536\n",
      "Loss in the iteration 581 is  4.209458\n",
      "Loss in the iteration 582 is  4.2013755\n",
      "Loss in the iteration 583 is  4.193312\n",
      "Loss in the iteration 584 is  4.1852636\n",
      "Loss in the iteration 585 is  4.177232\n",
      "Loss in the iteration 586 is  4.169215\n",
      "Loss in the iteration 587 is  4.1612163\n",
      "Loss in the iteration 588 is  4.153232\n",
      "Loss in the iteration 589 is  4.145264\n",
      "Loss in the iteration 590 is  4.1373115\n",
      "Loss in the iteration 591 is  4.129375\n",
      "Loss in the iteration 592 is  4.1214547\n",
      "Loss in the iteration 593 is  4.1135507\n",
      "Loss in the iteration 594 is  4.1056614\n",
      "Loss in the iteration 595 is  4.0977883\n",
      "Loss in the iteration 596 is  4.089931\n",
      "Loss in the iteration 597 is  4.0820894\n",
      "Loss in the iteration 598 is  4.074264\n",
      "Loss in the iteration 599 is  4.066454\n",
      "Loss in the iteration 600 is  4.058659\n",
      "Loss in the iteration 601 is  4.05088\n",
      "Loss in the iteration 602 is  4.0431166\n",
      "Loss in the iteration 603 is  4.035369\n",
      "Loss in the iteration 604 is  4.027636\n",
      "Loss in the iteration 605 is  4.019918\n",
      "Loss in the iteration 606 is  4.0122166\n",
      "Loss in the iteration 607 is  4.0045304\n",
      "Loss in the iteration 608 is  3.996859\n",
      "Loss in the iteration 609 is  3.9892042\n",
      "Loss in the iteration 610 is  3.9815629\n",
      "Loss in the iteration 611 is  3.9739377\n",
      "Loss in the iteration 612 is  3.9663293\n",
      "Loss in the iteration 613 is  3.9587338\n",
      "Loss in the iteration 614 is  3.951155\n",
      "Loss in the iteration 615 is  3.9435904\n",
      "Loss in the iteration 616 is  3.9360425\n",
      "Loss in the iteration 617 is  3.928508\n",
      "Loss in the iteration 618 is  3.920989\n",
      "Loss in the iteration 619 is  3.913487\n",
      "Loss in the iteration 620 is  3.9059966\n",
      "Loss in the iteration 621 is  3.898523\n",
      "Loss in the iteration 622 is  3.8910644\n",
      "Loss in the iteration 623 is  3.8836212\n",
      "Loss in the iteration 624 is  3.8761911\n",
      "Loss in the iteration 625 is  3.8687763\n",
      "Loss in the iteration 626 is  3.8613775\n",
      "Loss in the iteration 627 is  3.8539925\n",
      "Loss in the iteration 628 is  3.8466232\n",
      "Loss in the iteration 629 is  3.8392682\n",
      "Loss in the iteration 630 is  3.8319268\n",
      "Loss in the iteration 631 is  3.824602\n",
      "Loss in the iteration 632 is  3.8172905\n",
      "Loss in the iteration 633 is  3.809993\n",
      "Loss in the iteration 634 is  3.8027115\n",
      "Loss in the iteration 635 is  3.7954445\n",
      "Loss in the iteration 636 is  3.7881906\n",
      "Loss in the iteration 637 is  3.7809525\n",
      "Loss in the iteration 638 is  3.7737284\n",
      "Loss in the iteration 639 is  3.7665188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in the iteration 640 is  3.7593253\n",
      "Loss in the iteration 641 is  3.7521434\n",
      "Loss in the iteration 642 is  3.7449777\n",
      "Loss in the iteration 643 is  3.737825\n",
      "Loss in the iteration 644 is  3.7306876\n",
      "Loss in the iteration 645 is  3.7235641\n",
      "Loss in the iteration 646 is  3.716455\n",
      "Loss in the iteration 647 is  3.7093594\n",
      "Loss in the iteration 648 is  3.7022793\n",
      "Loss in the iteration 649 is  3.695212\n",
      "Loss in the iteration 650 is  3.68816\n",
      "Loss in the iteration 651 is  3.6811216\n",
      "Loss in the iteration 652 is  3.6740968\n",
      "Loss in the iteration 653 is  3.6670864\n",
      "Loss in the iteration 654 is  3.6600907\n",
      "Loss in the iteration 655 is  3.6531088\n",
      "Loss in the iteration 656 is  3.64614\n",
      "Loss in the iteration 657 is  3.6391845\n",
      "Loss in the iteration 658 is  3.6322436\n",
      "Loss in the iteration 659 is  3.6253169\n",
      "Loss in the iteration 660 is  3.6184044\n",
      "Loss in the iteration 661 is  3.6115055\n",
      "Loss in the iteration 662 is  3.6046207\n",
      "Loss in the iteration 663 is  3.5977495\n",
      "Loss in the iteration 664 is  3.5908916\n",
      "Loss in the iteration 665 is  3.5840478\n",
      "Loss in the iteration 666 is  3.577219\n",
      "Loss in the iteration 667 is  3.570401\n",
      "Loss in the iteration 668 is  3.5635982\n",
      "Loss in the iteration 669 is  3.5568078\n",
      "Loss in the iteration 670 is  3.5500326\n",
      "Loss in the iteration 671 is  3.54327\n",
      "Loss in the iteration 672 is  3.536521\n",
      "Loss in the iteration 673 is  3.5297863\n",
      "Loss in the iteration 674 is  3.5230634\n",
      "Loss in the iteration 675 is  3.5163555\n",
      "Loss in the iteration 676 is  3.5096612\n",
      "Loss in the iteration 677 is  3.5029802\n",
      "Loss in the iteration 678 is  3.4963112\n",
      "Loss in the iteration 679 is  3.4896553\n",
      "Loss in the iteration 680 is  3.4830146\n",
      "Loss in the iteration 681 is  3.4763858\n",
      "Loss in the iteration 682 is  3.4697711\n",
      "Loss in the iteration 683 is  3.4631693\n",
      "Loss in the iteration 684 is  3.45658\n",
      "Loss in the iteration 685 is  3.4500055\n",
      "Loss in the iteration 686 is  3.443442\n",
      "Loss in the iteration 687 is  3.4368927\n",
      "Loss in the iteration 688 is  3.4303575\n",
      "Loss in the iteration 689 is  3.4238338\n",
      "Loss in the iteration 690 is  3.4173238\n",
      "Loss in the iteration 691 is  3.410827\n",
      "Loss in the iteration 692 is  3.404343\n",
      "Loss in the iteration 693 is  3.397872\n",
      "Loss in the iteration 694 is  3.3914137\n",
      "Loss in the iteration 695 is  3.3849688\n",
      "Loss in the iteration 696 is  3.3785365\n",
      "Loss in the iteration 697 is  3.3721168\n",
      "Loss in the iteration 698 is  3.3657103\n",
      "Loss in the iteration 699 is  3.3593168\n",
      "Loss in the iteration 700 is  3.352935\n",
      "Loss in the iteration 701 is  3.3465674\n",
      "Loss in the iteration 702 is  3.3402116\n",
      "Loss in the iteration 703 is  3.333868\n",
      "Loss in the iteration 704 is  3.327538\n",
      "Loss in the iteration 705 is  3.3212206\n",
      "Loss in the iteration 706 is  3.3149154\n",
      "Loss in the iteration 707 is  3.3086236\n",
      "Loss in the iteration 708 is  3.3023438\n",
      "Loss in the iteration 709 is  3.2960765\n",
      "Loss in the iteration 710 is  3.2898211\n",
      "Loss in the iteration 711 is  3.2835793\n",
      "Loss in the iteration 712 is  3.27735\n",
      "Loss in the iteration 713 is  3.2711337\n",
      "Loss in the iteration 714 is  3.264929\n",
      "Loss in the iteration 715 is  3.2587354\n",
      "Loss in the iteration 716 is  3.2525563\n",
      "Loss in the iteration 717 is  3.2463882\n",
      "Loss in the iteration 718 is  3.2402332\n",
      "Loss in the iteration 719 is  3.2340906\n",
      "Loss in the iteration 720 is  3.2279594\n",
      "Loss in the iteration 721 is  3.2218413\n",
      "Loss in the iteration 722 is  3.2157345\n",
      "Loss in the iteration 723 is  3.2096412\n",
      "Loss in the iteration 724 is  3.2035582\n",
      "Loss in the iteration 725 is  3.1974888\n",
      "Loss in the iteration 726 is  3.1914318\n",
      "Loss in the iteration 727 is  3.1853857\n",
      "Loss in the iteration 728 is  3.1793528\n",
      "Loss in the iteration 729 is  3.1733315\n",
      "Loss in the iteration 730 is  3.167323\n",
      "Loss in the iteration 731 is  3.1613252\n",
      "Loss in the iteration 732 is  3.155339\n",
      "Loss in the iteration 733 is  3.1493661\n",
      "Loss in the iteration 734 is  3.1434047\n",
      "Loss in the iteration 735 is  3.1374557\n",
      "Loss in the iteration 736 is  3.1315181\n",
      "Loss in the iteration 737 is  3.125592\n",
      "Loss in the iteration 738 is  3.1196785\n",
      "Loss in the iteration 739 is  3.1137762\n",
      "Loss in the iteration 740 is  3.1078863\n",
      "Loss in the iteration 741 is  3.102008\n",
      "Loss in the iteration 742 is  3.0961409\n",
      "Loss in the iteration 743 is  3.0902863\n",
      "Loss in the iteration 744 is  3.084442\n",
      "Loss in the iteration 745 is  3.0786116\n",
      "Loss in the iteration 746 is  3.0727913\n",
      "Loss in the iteration 747 is  3.0669825\n",
      "Loss in the iteration 748 is  3.0611856\n",
      "Loss in the iteration 749 is  3.0554006\n",
      "Loss in the iteration 750 is  3.0496273\n",
      "Loss in the iteration 751 is  3.0438652\n",
      "Loss in the iteration 752 is  3.0381134\n",
      "Loss in the iteration 753 is  3.0323753\n",
      "Loss in the iteration 754 is  3.026648\n",
      "Loss in the iteration 755 is  3.0209324\n",
      "Loss in the iteration 756 is  3.0152278\n",
      "Loss in the iteration 757 is  3.0095353\n",
      "Loss in the iteration 758 is  3.0038538\n",
      "Loss in the iteration 759 is  2.9981825\n",
      "Loss in the iteration 760 is  2.9925237\n",
      "Loss in the iteration 761 is  2.9868762\n",
      "Loss in the iteration 762 is  2.9812398\n",
      "Loss in the iteration 763 is  2.9756138\n",
      "Loss in the iteration 764 is  2.9700015\n",
      "Loss in the iteration 765 is  2.9643972\n",
      "Loss in the iteration 766 is  2.9588053\n",
      "Loss in the iteration 767 is  2.9532256\n",
      "Loss in the iteration 768 is  2.9476566\n",
      "Loss in the iteration 769 is  2.9420974\n",
      "Loss in the iteration 770 is  2.93655\n",
      "Loss in the iteration 771 is  2.931014\n",
      "Loss in the iteration 772 is  2.9254906\n",
      "Loss in the iteration 773 is  2.9199755\n",
      "Loss in the iteration 774 is  2.9144728\n",
      "Loss in the iteration 775 is  2.9089813\n",
      "Loss in the iteration 776 is  2.9035\n",
      "Loss in the iteration 777 is  2.89803\n",
      "Loss in the iteration 778 is  2.8925722\n",
      "Loss in the iteration 779 is  2.8871243\n",
      "Loss in the iteration 780 is  2.8816862\n",
      "Loss in the iteration 781 is  2.87626\n",
      "Loss in the iteration 782 is  2.8708444\n",
      "Loss in the iteration 783 is  2.8654406\n",
      "Loss in the iteration 784 is  2.8600461\n",
      "Loss in the iteration 785 is  2.8546624\n",
      "Loss in the iteration 786 is  2.8492904\n",
      "Loss in the iteration 787 is  2.8439293\n",
      "Loss in the iteration 788 is  2.8385782\n",
      "Loss in the iteration 789 is  2.8332374\n",
      "Loss in the iteration 790 is  2.827909\n",
      "Loss in the iteration 791 is  2.8225894\n",
      "Loss in the iteration 792 is  2.817282\n",
      "Loss in the iteration 793 is  2.8119838\n",
      "Loss in the iteration 794 is  2.8066962\n",
      "Loss in the iteration 795 is  2.8014193\n",
      "Loss in the iteration 796 is  2.7961543\n",
      "Loss in the iteration 797 is  2.790899\n",
      "Loss in the iteration 798 is  2.785655\n",
      "Loss in the iteration 799 is  2.7804203\n",
      "Loss in the iteration 800 is  2.7751963\n",
      "Loss in the iteration 801 is  2.769983\n",
      "Loss in the iteration 802 is  2.76478\n",
      "Loss in the iteration 803 is  2.7595868\n",
      "Loss in the iteration 804 is  2.7544043\n",
      "Loss in the iteration 805 is  2.7492325\n",
      "Loss in the iteration 806 is  2.7440712\n",
      "Loss in the iteration 807 is  2.7389193\n",
      "Loss in the iteration 808 is  2.73378\n",
      "Loss in the iteration 809 is  2.7286494\n",
      "Loss in the iteration 810 is  2.7235284\n",
      "Loss in the iteration 811 is  2.7184188\n",
      "Loss in the iteration 812 is  2.713318\n",
      "Loss in the iteration 813 is  2.708228\n",
      "Loss in the iteration 814 is  2.703149\n",
      "Loss in the iteration 815 is  2.6980803\n",
      "Loss in the iteration 816 is  2.6930206\n",
      "Loss in the iteration 817 is  2.6879718\n",
      "Loss in the iteration 818 is  2.6829317\n",
      "Loss in the iteration 819 is  2.6779025\n",
      "Loss in the iteration 820 is  2.6728845\n",
      "Loss in the iteration 821 is  2.6678762\n",
      "Loss in the iteration 822 is  2.6628768\n",
      "Loss in the iteration 823 is  2.6578877\n",
      "Loss in the iteration 824 is  2.6529093\n",
      "Loss in the iteration 825 is  2.64794\n",
      "Loss in the iteration 826 is  2.6429803\n",
      "Loss in the iteration 827 is  2.6380312\n",
      "Loss in the iteration 828 is  2.6330922\n",
      "Loss in the iteration 829 is  2.6281624\n",
      "Loss in the iteration 830 is  2.6232433\n",
      "Loss in the iteration 831 is  2.6183336\n",
      "Loss in the iteration 832 is  2.6134331\n",
      "Loss in the iteration 833 is  2.6085439\n",
      "Loss in the iteration 834 is  2.603663\n",
      "Loss in the iteration 835 is  2.5987928\n",
      "Loss in the iteration 836 is  2.593932\n",
      "Loss in the iteration 837 is  2.5890818\n",
      "Loss in the iteration 838 is  2.5842402\n",
      "Loss in the iteration 839 is  2.5794082\n",
      "Loss in the iteration 840 is  2.5745857\n",
      "Loss in the iteration 841 is  2.5697732\n",
      "Loss in the iteration 842 is  2.5649714\n",
      "Loss in the iteration 843 is  2.5601783\n",
      "Loss in the iteration 844 is  2.5553942\n",
      "Loss in the iteration 845 is  2.5506198\n",
      "Loss in the iteration 846 is  2.545855\n",
      "Loss in the iteration 847 is  2.5410998\n",
      "Loss in the iteration 848 is  2.5363543\n",
      "Loss in the iteration 849 is  2.5316188\n",
      "Loss in the iteration 850 is  2.5268924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in the iteration 851 is  2.5221758\n",
      "Loss in the iteration 852 is  2.517468\n",
      "Loss in the iteration 853 is  2.5127692\n",
      "Loss in the iteration 854 is  2.508081\n",
      "Loss in the iteration 855 is  2.503401\n",
      "Loss in the iteration 856 is  2.4987311\n",
      "Loss in the iteration 857 is  2.494071\n",
      "Loss in the iteration 858 is  2.489419\n",
      "Loss in the iteration 859 is  2.484777\n",
      "Loss in the iteration 860 is  2.480144\n",
      "Loss in the iteration 861 is  2.4755208\n",
      "Loss in the iteration 862 is  2.4709063\n",
      "Loss in the iteration 863 is  2.4663012\n",
      "Loss in the iteration 864 is  2.461705\n",
      "Loss in the iteration 865 is  2.4571185\n",
      "Loss in the iteration 866 is  2.452541\n",
      "Loss in the iteration 867 is  2.4479723\n",
      "Loss in the iteration 868 is  2.4434135\n",
      "Loss in the iteration 869 is  2.438863\n",
      "Loss in the iteration 870 is  2.4343226\n",
      "Loss in the iteration 871 is  2.42979\n",
      "Loss in the iteration 872 is  2.425268\n",
      "Loss in the iteration 873 is  2.420754\n",
      "Loss in the iteration 874 is  2.4162495\n",
      "Loss in the iteration 875 is  2.4117534\n",
      "Loss in the iteration 876 is  2.4072657\n",
      "Loss in the iteration 877 is  2.402788\n",
      "Loss in the iteration 878 is  2.3983185\n",
      "Loss in the iteration 879 is  2.393859\n",
      "Loss in the iteration 880 is  2.3894079\n",
      "Loss in the iteration 881 is  2.3849657\n",
      "Loss in the iteration 882 is  2.3805325\n",
      "Loss in the iteration 883 is  2.3761075\n",
      "Loss in the iteration 884 is  2.371692\n",
      "Loss in the iteration 885 is  2.3672853\n",
      "Loss in the iteration 886 is  2.3628871\n",
      "Loss in the iteration 887 is  2.358498\n",
      "Loss in the iteration 888 is  2.3541176\n",
      "Loss in the iteration 889 is  2.3497455\n",
      "Loss in the iteration 890 is  2.345383\n",
      "Loss in the iteration 891 is  2.3410292\n",
      "Loss in the iteration 892 is  2.3366835\n",
      "Loss in the iteration 893 is  2.332347\n",
      "Loss in the iteration 894 is  2.328018\n",
      "Loss in the iteration 895 is  2.323699\n",
      "Loss in the iteration 896 is  2.319388\n",
      "Loss in the iteration 897 is  2.3150852\n",
      "Loss in the iteration 898 is  2.3107924\n",
      "Loss in the iteration 899 is  2.3065069\n",
      "Loss in the iteration 900 is  2.30223\n",
      "Loss in the iteration 901 is  2.297962\n",
      "Loss in the iteration 902 is  2.293703\n",
      "Loss in the iteration 903 is  2.289451\n",
      "Loss in the iteration 904 is  2.2852087\n",
      "Loss in the iteration 905 is  2.2809753\n",
      "Loss in the iteration 906 is  2.2767496\n",
      "Loss in the iteration 907 is  2.2725322\n",
      "Loss in the iteration 908 is  2.2683234\n",
      "Loss in the iteration 909 is  2.2641244\n",
      "Loss in the iteration 910 is  2.2599323\n",
      "Loss in the iteration 911 is  2.255748\n",
      "Loss in the iteration 912 is  2.2515738\n",
      "Loss in the iteration 913 is  2.2474065\n",
      "Loss in the iteration 914 is  2.2432487\n",
      "Loss in the iteration 915 is  2.2390985\n",
      "Loss in the iteration 916 is  2.2349558\n",
      "Loss in the iteration 917 is  2.2308228\n",
      "Loss in the iteration 918 is  2.2266972\n",
      "Loss in the iteration 919 is  2.2225802\n",
      "Loss in the iteration 920 is  2.2184718\n",
      "Loss in the iteration 921 is  2.2143712\n",
      "Loss in the iteration 922 is  2.2102783\n",
      "Loss in the iteration 923 is  2.2061937\n",
      "Loss in the iteration 924 is  2.2021174\n",
      "Loss in the iteration 925 is  2.1980493\n",
      "Loss in the iteration 926 is  2.1939907\n",
      "Loss in the iteration 927 is  2.189938\n",
      "Loss in the iteration 928 is  2.1858954\n",
      "Loss in the iteration 929 is  2.181859\n",
      "Loss in the iteration 930 is  2.1778328\n",
      "Loss in the iteration 931 is  2.1738124\n",
      "Loss in the iteration 932 is  2.1698012\n",
      "Loss in the iteration 933 is  2.1657982\n",
      "Loss in the iteration 934 is  2.161803\n",
      "Loss in the iteration 935 is  2.1578162\n",
      "Loss in the iteration 936 is  2.1538365\n",
      "Loss in the iteration 937 is  2.1498647\n",
      "Loss in the iteration 938 is  2.1459012\n",
      "Loss in the iteration 939 is  2.1419463\n",
      "Loss in the iteration 940 is  2.1379983\n",
      "Loss in the iteration 941 is  2.1340582\n",
      "Loss in the iteration 942 is  2.1301265\n",
      "Loss in the iteration 943 is  2.1262033\n",
      "Loss in the iteration 944 is  2.1222873\n",
      "Loss in the iteration 945 is  2.1183786\n",
      "Loss in the iteration 946 is  2.1144779\n",
      "Loss in the iteration 947 is  2.1105855\n",
      "Loss in the iteration 948 is  2.1067007\n",
      "Loss in the iteration 949 is  2.102823\n",
      "Loss in the iteration 950 is  2.0989535\n",
      "Loss in the iteration 951 is  2.0950918\n",
      "Loss in the iteration 952 is  2.0912383\n",
      "Loss in the iteration 953 is  2.0873916\n",
      "Loss in the iteration 954 is  2.0835533\n",
      "Loss in the iteration 955 is  2.0797224\n",
      "Loss in the iteration 956 is  2.0758996\n",
      "Loss in the iteration 957 is  2.072084\n",
      "Loss in the iteration 958 is  2.0682762\n",
      "Loss in the iteration 959 is  2.0644755\n",
      "Loss in the iteration 960 is  2.0606835\n",
      "Loss in the iteration 961 is  2.0568986\n",
      "Loss in the iteration 962 is  2.0531204\n",
      "Loss in the iteration 963 is  2.0493503\n",
      "Loss in the iteration 964 is  2.045588\n",
      "Loss in the iteration 965 is  2.0418324\n",
      "Loss in the iteration 966 is  2.0380857\n",
      "Loss in the iteration 967 is  2.0343459\n",
      "Loss in the iteration 968 is  2.0306132\n",
      "Loss in the iteration 969 is  2.0268886\n",
      "Loss in the iteration 970 is  2.023171\n",
      "Loss in the iteration 971 is  2.0194607\n",
      "Loss in the iteration 972 is  2.0157585\n",
      "Loss in the iteration 973 is  2.012063\n",
      "Loss in the iteration 974 is  2.008374\n",
      "Loss in the iteration 975 is  2.0046935\n",
      "Loss in the iteration 976 is  2.0010207\n",
      "Loss in the iteration 977 is  1.9973544\n",
      "Loss in the iteration 978 is  1.9936961\n",
      "Loss in the iteration 979 is  1.9900448\n",
      "Loss in the iteration 980 is  1.9864004\n",
      "Loss in the iteration 981 is  1.9827632\n",
      "Loss in the iteration 982 is  1.9791337\n",
      "Loss in the iteration 983 is  1.9755114\n",
      "Loss in the iteration 984 is  1.971896\n",
      "Loss in the iteration 985 is  1.9682884\n",
      "Loss in the iteration 986 is  1.9646878\n",
      "Loss in the iteration 987 is  1.9610947\n",
      "Loss in the iteration 988 is  1.9575088\n",
      "Loss in the iteration 989 is  1.9539297\n",
      "Loss in the iteration 990 is  1.9503572\n",
      "Loss in the iteration 991 is  1.9467925\n",
      "Loss in the iteration 992 is  1.9432356\n",
      "Loss in the iteration 993 is  1.9396853\n",
      "Loss in the iteration 994 is  1.9361422\n",
      "Loss in the iteration 995 is  1.9326065\n",
      "Loss in the iteration 996 is  1.9290769\n",
      "Loss in the iteration 997 is  1.9255546\n",
      "Loss in the iteration 998 is  1.9220399\n",
      "Loss in the iteration 999 is  1.9185319\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    _, curr_loss = sess.run([optimize, mean_loss],\n",
    "                            feed_dict = {inputs: train_data['inputs'], targets: train_data['targets']})\n",
    "    \n",
    "    print(\"Loss in the iteration %d is \"%epoch, curr_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = sess.run([outputs], feed_dict={inputs: train_data['inputs']})\n",
    "plt.plot(np.squeeze(out), np.squeeze(train_data['targets']))\n",
    "plt.xlabel('output')\n",
    "plt.ylabel('target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
